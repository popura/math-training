\section{線形代数の基礎}

\subsection{連立方程式\label{linalg:linear_system}}
  \vspace{1mm}
  \begin{enumerate}[label=(\roman*)]
    \item
    拡大係数行列$\tilde{\mathrm{A}} = (\mathrm{A}|b)$に対し，拡大係数行列の左側が以下のような行列
    (対角に相当する成分のみ1を含み，他の成分がすべて0である行列) となるよう，
    行基本変形を施す．
    \begin{equation}
      \left(
        \begin{array}{rrrrrrr}
          1 &   &        &   &   &        & 0 \\
            & 1 &        &   &   &        &   \\
            &   & \ddots &   &   &        &   \\
            &   &        & 1 &   &        &   \\
            &   &        &   & 0 &        &   \\
            &   &        &   &   & \ddots &   \\
          0 &   &        &   &   &        & 0 \\
        \end{array}
      \right)
    \end{equation}
    この結果得られる拡大係数行列の右側が元の方程式の解に相当する．
    よって解$x$は，
    \begin{equation}
      x = \left(
        \begin{array}{r}
          6 \\ 2 \\ 3
        \end{array}
        \right)
    \end{equation}
    である．
    
    \item
    \begin{itemize}
      \item (i)と同様に拡大係数行列$\tilde{\mathrm{A}} = (\mathrm{A}|b)$を変形すると以下を得る
        (変形を途中でやめていることに注意)．
        \begin{equation}
          \left(
            \begin{array}{rr|r}
              1 & 1 & 1 \\
              0 & 1 & 0 \\
              0 & 0 & 2 \\
            \end{array}
          \right)
        \end{equation}
        このことから，
        $\rank(\mathrm{A}) \neq \rank(\tilde{\mathrm{A}})$であり，
        $Ax - b = 0$は解を持たない．
      \item $\|\mathrm{A}x-b\|$ を最小とする $x$ は，
      $\|\mathrm{A}x-b\|^2$ を最小とする $x$ と等しい．
      よって，$\|\mathrm{A}x-b\|^2$ を最小とする $x$ を考える．
      $\|\mathrm{A}x-b\|^2$は二次式だから，
      \begin{equation}
        \frac{\partial}{\partial x} \| \mathrm{A}x-b \|^2 = 0
      \end{equation}
      を満たす$x$を求めればよい．
      上式を解くと，
      \begin{equation}
        x = (\mathrm{A}^\top \mathrm{A})^{-1} \mathrm{A}^\top b
      \end{equation}
      を得る．
      したがって，
      \begin{equation}
        x = \left(
          \begin{array}{r}
             1 \\
             \frac{2}{3} \\
          \end{array}
          \right).
      \end{equation}
      \begin{enumerate}[label=別解\arabic*]
        \item $\|\mathrm{A}x-b\|^2$を$x$の各要素について平方完成して最小値を求める
        \item $\|\mathrm{A}x-b\|^2$を$x$の各要素について微分して，
          \begin{equation}
            \frac{\partial}{\partial x_1} \| \mathrm{A}x-b \|^2
            = \frac{\partial}{\partial x_2} \| \mathrm{A}x-b \|^2 = 0
          \end{equation}
          を解き，最小値を求める．
      \end{enumerate}
    \end{itemize}

    \item
    \begin{itemize}
      \item (i)と同様に拡大係数行列$\tilde{\mathrm{A}} = (\mathrm{A}|b)$を変形すると以下を得る．
        \begin{equation}
          \left(
            \begin{array}{rrrr|r}
              1 & 0 & 0 &  2 & -1 \\
              0 & 1 & 0 & -1 &  1 \\
              0 & 0 & 1 & -1 & -1 \\
              0 & 0 & 0 &  0 &  0 \\
            \end{array}
          \right)
        \end{equation}
        よって，解$x$は，任意の$t \in \mathbb{R}$を用いて
        \begin{equation}
          x = t
            \left(
            \begin{array}{r}
              -2 \\ 1 \\ 1 \\ 1
            \end{array}
            \right)
            +
            \left(
            \begin{array}{r}
              -1 \\ 1 \\ -1 \\ 0
            \end{array}
            \right)
        \end{equation}
        として与えられる．
      \item $\|x\|$ を最小とする $x$ は，
      $\|x\|^2$ を最小とする $x$ と等しい．
      よって，$\|x\|^2$ を最小とする $x$ を考える．
      $\|x\|^2$は二次式だから，
      \begin{equation}
        \frac{d}{dt} \| x \|^2 = 0
      \end{equation}
      を満たす$x$を求めればよい．
      上式を解くと，
      \begin{equation}
        t = -\frac{2}{7}
      \end{equation}
      を得る．
      したがって，
      \begin{equation}
        x = -\frac{1}{7}
          \left(
          \begin{array}{r}
              3 \\
             -5 \\
              9 \\
              2
          \end{array}
          \right).
      \end{equation}
      \begin{enumerate}[label=別解\arabic*]
        \item $\|\mathrm{A}x-b\|^2$を$t$について平方完成して最小値を求める
      \end{enumerate}
    \end{itemize}
  \end{enumerate}

\clearpage
\subsection{線形変換}
  \vspace{1mm}
  \begin{enumerate}[label=(\roman*)]
    \item $\vecsym{y} = a \vecsym{x} + \vecsym{x}_\perp$より，
      $\vecsym{x}^\top \vecsym{y} = \vecsym{x}^\top (a \vecsym{x} + \vecsym{x}_\perp) = a \| \vecsym{x} \|^2$．
      $a$について整理すると，
      \begin{equation}
        a = \frac{1}{\| \vecsym{x} \|^2} \vecsym{x}^\top \vecsym{y}
      \end{equation}
      を得る．
    \item $\matsym{A} \vecsym{y} = a \vecsym{x}$に(i)の結果を代入することで，
      \begin{equation}
        \matsym{A} = \frac{1}{\| \vecsym{x} \|^2} \vecsym{x} \vecsym{x}^\top
      \end{equation}
      を得る．
  \end{enumerate}

\clearpage
\subsection{行列のランク}
  \vspace{1mm}
  \begin{enumerate}[label=(\roman*)]
    \item a
    \item b
    \item c
    \item d
    \item e
  \end{enumerate}

\clearpage
\subsection{行列式と逆行列の存在条件}
  \vspace{1mm}
  \begin{enumerate}[label=(\roman*)]
    \item
      \begin{align}
        \det{(\matsym{V})} &=
          \begin{vmatrix}
            1 & 1 & 1 & 1 \\
            x_1 & x_2 & x_3 & x_4 \\
            x_1^2 & x_2^2 & x_3^2 & x_4^2 \\
            x_1^3 & x_2^3 & x_3^3 & x_4^3
          \end{vmatrix}
          =
          \begin{vmatrix}
            1 & 1 & 1 & 1 \\
            0 & x_2 - x_1 & x_3 - x_1 & x_4 - x_1 \\
            0 & x_2 (x_2 - x_1) & x_3 (x_3 - x_1) & x_4 (x_4 - x_1)\\
            0 & x_2^2 (x_2 - x_1) & x_3^2 (x_3 - x_1) & x_4^2 (x_4 - x_1) 
          \end{vmatrix}\\
          &=
          \begin{vmatrix}
            x_2 - x_1 & x_3 - x_1 & x_4 - x_1 \\
            x_2 (x_2 - x_1) & x_3 (x_3 - x_1) & x_4 (x_4 - x_1)\\
            x_2^2 (x_2 - x_1) & x_3^2 (x_3 - x_1) & x_4^2 (x_4 - x_1) 
          \end{vmatrix}\\
          &= (x_2 - x_1) (x_3 - x_1) (x_4 - x_1)
          \begin{vmatrix}
            1 & 1 & 1 \\
            x_2 & x_3 & x_4\\
            x_2^2 & x_3^2 & x_4^2
          \end{vmatrix}\\
          &= (x_2 - x_1) (x_3 - x_1) (x_4 - x_1)
          \begin{vmatrix}
            1 & 1 & 1 \\
            0 & x_3 - x_2 & x_4 - x_2\\
            0 & x_3 (x_3 - x_2) & x_4 (x_3 - x_2)
          \end{vmatrix}\\
          &= (x_2 - x_1) (x_3 - x_1) (x_4 - x_1) (x_3 - x_2) (x_4 - x_2)
          \begin{vmatrix}
            1 & 1 \\
            x_3 & x_4
          \end{vmatrix}\\
          &= (x_2 - x_1) (x_3 - x_1) (x_4 - x_1) (x_3 - x_2) (x_4 - x_2) (x_4 - x_3)
      \end{align}
    \item 行列$\matsym{V}$が逆行列を持つための必要十分条件は，$\det{(\matsym{V})} \neq 0$であることである．
      \begin{equation}
        \det{(\matsym{V})} = (x_2 - x_1) (x_3 - x_1) (x_4 - x_1) (x_3 - x_2) (x_4 - x_2) (x_4 - x_3) \neq 0
      \end{equation}
      より，
      $i, j \in \{1, 2, 3, 4\}$かつ$i \neq j$であるすべての$(i, j)$について$x_i \neq x_j$を満たすとき，
      行列$\matsym{V}$は逆行列を持つ．
  \end{enumerate}
      
\clearpage
\subsection{回転行列}
  $\vecsym{e}_1 = \begin{pmatrix} 1 \\ 0 \end{pmatrix}$,
  $\vecsym{e}_2 = \begin{pmatrix} 0 \\ 1 \end{pmatrix}$
  とする．
  \begin{enumerate}[label=(\roman*)]
    \item $\vecsym{e}'_1 = \begin{pmatrix} \cos\theta \\ \sin\theta \end{pmatrix}$,
      $\vecsym{e}'_2 = \begin{pmatrix} -\sin\theta \\ \cos\theta \end{pmatrix}$
    \item 
      \begin{equation}
        \begin{pmatrix} \vecsym{e}'_1 & \vecsym{e}'_2 \end{pmatrix}
        = \matsym{R}_{\theta} \begin{pmatrix} \vecsym{e}_1 & \vecsym{e}_2 \end{pmatrix}
      \end{equation}
      より，
      \begin{equation}
        \begin{pmatrix}
          \cos\theta & -\sin\theta \\
          \sin\theta & \cos\theta
        \end{pmatrix}
        = \matsym{R}_{\theta}
        \begin{pmatrix}
          1 & 0 \\
          0 & 1
        \end{pmatrix}.
      \end{equation}
      したがって，
      \begin{equation}
        \matsym{R}_\theta =
          \begin{pmatrix}
            \cos\theta & -\sin\theta \\
            \sin\theta & \cos\theta
          \end{pmatrix}
      \end{equation}
    \item
      \begin{align}
        \vecsym{\nu} &= \matsym{R}_\phi \matsym{R}_\theta \vecsym{e}_1 \\
          &=
          \begin{pmatrix}
            \cos\theta & -\sin\theta \\
            \sin\theta & \cos\theta
          \end{pmatrix}
          \begin{pmatrix}
            \cos\theta & -\sin\theta \\
            \sin\theta & \cos\theta
          \end{pmatrix}
          \vecsym{e}_1\\
          &=
          \begin{pmatrix}
            \cos\theta \cos\phi - \sin\theta \sin\phi \\
            \sin\theta \cos\phi + \cos\theta \sin\phi
          \end{pmatrix}
      \end{align}
    \item
      \begin{equation}
        \matsym{R}_{-\theta} 
          =
          \begin{pmatrix}
            \cos\theta  & \sin\theta \\
            -\sin\theta & \cos\theta
          \end{pmatrix}
      \end{equation}
      であり，
      \begin{equation}
        \matsym{R}_\theta \matsym{R}_{-\theta} = \matsym{R}_{\theta} \matsym{R}_\theta = \matsym{I}
      \end{equation}
      （$\matsym{I}$は単位行列）
      より，$\matsym{R}_\theta^{-1} = \matsym{R}_{-\theta}$.
  \end{enumerate}


\clearpage
\subsection{エルミート行列}
  \begin{enumerate}[label=(\roman*)]
    \item 任意の行列$\matsym{M} \in \numset{C}^{n \times m}$について，
      $\matsym{M}\htp{\matsym{M}}$および
      $\htp{\matsym{M}}\matsym{M}$
      がエルミート行列となることを証明する．
      $\htp{(\matsym{M}\tilde{\matsym{M}})} = \htp{\tilde{\matsym{M}}}\htp{\matsym{M}}$より，
      $\htp{(\matsym{M}\htp{\matsym{M}})} = \htp{(\htp{\matsym{M}})}\htp{\matsym{M}} = \matsym{M}\htp{\matsym{M}}$
      であるため，$\matsym{M}\htp{\matsym{M}}$はエルミート行列．
      同様に，$\htp{\matsym{M}}\matsym{M}$もエルミート行列．
      
    \item 任意のエルミート行列$\matsym{A} \in \numset{C}^{n \times n}$について，
      すべての固有値が実数となることを証明する．
      定義より，行列$\matsym{A}$の固有値$\lambda$と固有ベクトル$\vecsym{v}$は，
      $\matsym{A}\vecsym{v} = \lambda \vecsym{v}$を満たす．
      よって，
      \begin{align}
        \htp{vecsym{v}} \matsym{A} \vecsym{v}
        = \htp{vecsym{v}} \lambda \vecsym{v}
        = \lambda \htp{vecsym{v}} \vecsym{v}.
      \end{align}
      また，
      \begin{align}
        \htp{vecsym{v}} \htp{\matsym{A}} \vecsym{v}
        = \htp{(\matsym{A} vecsym{v})} \vecsym{v}
        = \htp{(\lambda vecsym{v})} \vecsym{v}
        = \overline{\lambda} \htp{vecsym{v}} \vecsym{v}.
      \end{align}
      したがって，$\lambda = \overline{\lambda}$でなければならないため，
      $\lambda$は実数．
    
    \item 任意のエルミート行列$\matsym{A} \in \numset{C}^{n \times n}$について，
      $\matsym{A}$があるユニタリ行列$\matsym{U}$で対角化されることを証明する．
      
  \end{enumerate}

  \clearpage
  \subsection{特異値分解}
    \begin{enumerate}[label=(\roman*)]
      \item $\matsym{A} = \matsym{U}\matsym{\Sigma}\htp{\matsym{V}}$より，
        \begin{align}
          \matsym{A}\htp{\matsym{A}} &= \matsym{U}\matsym{\Sigma}\htp{\matsym{V}}\htp{(\matsym{U}\matsym{\Sigma}\htp{\matsym{V}})} \\
            &= \matsym{U}\matsym{\Sigma}\htp{\matsym{V}}\matsym{V}\htp{\matsym{\Sigma}}\htp{\matsym{U}}.
        \end{align}
        ここで，$\matsym{V}$はユニタリ行列だから，
        $\htp{\matsym{V}}\matsym{V} = \matsym{I}$ ($\matsym{I}$は単位行列)．
        よって，
        \begin{equation}
          \matsym{A}\htp{\matsym{A}} = \matsym{U}\matsym{\Sigma}\htp{\matsym{\Sigma}}\htp{\matsym{U}}.
        \end{equation}
        $\matsym{\Sigma}\htp{\matsym{\Sigma}}$は対角行列となるため，
        $\matsym{\Sigma}\htp{\matsym{\Sigma}} = \matsym{\Lambda}$とおけば，
        \begin{equation}
          \matsym{A}\htp{\matsym{A}} = \matsym{U}\matsym{\Lambda}\htp{\matsym{U}}
        \end{equation}
        であり，これは，$\matsym{A}\htp{\matsym{A}}$の固有値分解である．
        よって$\matsym{U}$は，$\matsym{A}\htp{\matsym{A}}$の
        （ノルムが1に正規化された）固有ベクトルを並べた行列として与えられる．
      \item 同様に，
        $\htp{\matsym{\Sigma}}\matsym{\Sigma} = \matsym{\Lambda}'$とおけば，
        \begin{equation}
          \htp{\matsym{A}}\matsym{A} = \matsym{V}\matsym{\Lambda}'\htp{\matsym{V}}
        \end{equation}
        であり，これは，$\htp{\matsym{A}}\matsym{A}$の固有値分解である．
        よって$\matsym{V}$は，$\htp{\matsym{A}}\matsym{A}$の
        （ノルムが1に正規化された）固有ベクトルを並べた行列として与えられる．
      \item $\matsym{\Sigma}\htp{\matsym{\Sigma}} = \matsym{\Lambda}$であり，
        $\matsym{\Lambda}$の$(i, i)$要素は$\matsym{\Sigma}$の$(i, i)$要素の自乗である．
        また，
        $\matsym{\Lambda}$は$\matsym{A}\htp{\matsym{A}}$の固有値を対角に並べた行列である．
        したがって，$\matsym{\Sigma}$は，
        $\matsym{A}\htp{\matsym{A}}$の固有値の平方根を対角並べた行列となる．
      \item $\matsym{A}$が縦長行列であるため，
        $\matsym{A}\htp{\matsym{A}}$の固有値分解より
        $\htp{\matsym{A}}\matsym{A}$の固有値分解のほうが簡単である．
        $\htp{\matsym{A}}\matsym{A}$の固有値分解のほうが簡単である．
        \begin{align}
          \htp{\matsym{A}}\matsym{A}
            &= 
              \begin{pmatrix}
                1 & 1 & 0 \\
                1 & -1 & 1
              \end{pmatrix}
              \begin{pmatrix}
                1 & 1 \\
                1 & -1 \\
                0 & 1
              \end{pmatrix} \\
            &=
              \begin{pmatrix}
                2 & 0 \\
                0 & 3 
              \end{pmatrix}
        \end{align}
        より，固有値は$2, 3$であり，
        対応する固有ベクトルはそれぞれ$\tp{(a, 0)}, \tp{(0, b)}$である
        （ただし，$a, b$は任意のスカラー）．
        よって，$a = b = 1$とすれば，
        \begin{equation}
          \matsym{V} = 
          \begin{pmatrix}
            1 & 0 \\
            0 & 1 
          \end{pmatrix},
          \matsym{\Sigma} =
          \begin{pmatrix}
            \sqrt{2} & 0 \\
            0 & \sqrt{3} \\
            0 & 0
          \end{pmatrix}
        \end{equation}
        を得る．
        また，$\matsym{A} = \matsym{U}\matsym{\Sigma}\htp{\matsym{V}}$より，
        $\matsym{U} = \matsym{A}\htp{\matsym{V}}$

    \end{enumerate}

  \clearpage
  \subsection{Moore-Penroseの擬似逆行列}
    方程式$\matsym{A}\vecsym{x} = \vecsym{b}$は，
    $\matsym{A}$が正則行列であれば，その逆行列を用いて
    $\vecsym{x} = \matsym{A}^{-1}\vecsym{b}$として解くことができる．
    任意の行列$\matsym{A}$については，
    その擬似逆行列を用いて
    $\vecsym{x} = \pinv{\matsym{A}}\vecsym{b}$として一つの（近似）解を得ることができる．
    その解は，
    \begin{itemize}[nosep]
      \item 方程式$\matsym{A}\vecsym{x} = \vecsym{b}$が決定系のときには，
        その解$\vecsym{x}$
      \item 方程式$\matsym{A}\vecsym{x} = \vecsym{b}$が優決定系のときには，
        $\|\matsym{A} \vecsym{x} - \vecsym{b} \|$ が最小となる $\vecsym{x}$
      \item 方程式$\matsym{A}\vecsym{x} = \vecsym{b}$が劣決定系のときには，
        $\matsym{A} \vecsym{x} - \vecsym{b} = \vecsym{0}$の解$\vecsym{x}$のうち，$\|\vecsym{x}\|$が最小となる$\vecsym{x}$
    \end{itemize}
    に一致する．
    
    \begin{enumerate}[label=(\roman*)]
      \item
        \begin{align}
          \pinv{\matsym{A}} &=
            \begin{pmatrix}
              \dfrac{1}{2} & \dfrac{1}{2} & 0 \\[1.5ex]
              \dfrac{1}{3} & -\dfrac{1}{3} & \dfrac{1}{3}
            \end{pmatrix} \\
          \pinv{\matsym{A}} \vecsym{b} &=
            \begin{pmatrix}
              1 \\[1.5ex]
              \dfrac{2}{3}
            \end{pmatrix} \\
        \end{align}
        よって，
        \ref{linalg:linear_system}(ii)で計算した$\vecsym{x}$と等しい．
      
      \item
        \begin{align}
          \pinv{\matsym{A}} &=
            \frac{1}{735}
            \begin{pmatrix}
               23 &  -26 & 143 &   9 \\
               97 &  146 & -68 &   6 \\
              -43 & -239 & 212 & 111 \\
              -8  &   41 & 142 & -99
            \end{pmatrix}\\
          \pinv{\matsym{A}} \vecsym{b} &=
            \frac{1}{7}
            \begin{pmatrix}
              -3 \\
               5 \\
              -9 \\
              -2 \\
            \end{pmatrix} \\
        \end{align}
        よって，
        \ref{linalg:linear_system}(iii)で計算した$\vecsym{x}$と等しい．
    \end{enumerate}
  

\clearpage
\subsection{離散畳み込みとDFT}
  \begin{enumerate}[label=(\roman*)]
    \item 
      \begin{equation}
        \matsym{L} =
          \begin{pmatrix}
            a_0 &   0 &   0 &   0 \\
            a_1 & a_0 &   0 &   0 \\
            a_2 & a_1 & a_0 &   0 \\
            a_3 & a_2 & a_1 & a_0
          \end{pmatrix}.
      \end{equation}
      線形畳み込みを行列表現するとテプリッツ行列となる．
    \item 
      \begin{equation}
        \matsym{C} =
          \begin{pmatrix}
            a_0 & a_3 & a_2 & a_1 \\
            a_1 & a_0 & a_3 & a_2 \\
            a_2 & a_1 & a_0 & a_3 \\
            a_3 & a_2 & a_1 & a_0
          \end{pmatrix}.
      \end{equation}
      巡回畳み込みを行列表現すると巡回行列となる．
    \item 
      \begin{align}
        \matsym{F}\matsym{C}\matsym{F}^{-1} &= \frac{1}{4} \matsym{F}\matsym{C}\htp{\matsym{F}} \\
          &= 
            \frac{1}{4}
            \begin{pmatrix}
              1 &  1 &  1&  1\\
              1 &  j & -1& -j\\
              1 & -1 &  1& -1\\
              1 & -j & -1&  j
            \end{pmatrix}
            \begin{pmatrix}
              a_0 & a_3 & a_2 & a_1 \\
              a_1 & a_0 & a_3 & a_2 \\
              a_2 & a_1 & a_0 & a_3 \\
              a_3 & a_2 & a_1 & a_0
            \end{pmatrix}.
            \begin{pmatrix}
              1 &  1 &  1&  1\\
              1 & -j & -1&  j\\
              1 & -1 &  1& -1\\
              1 &  j & -1& -j
            \end{pmatrix}
          \\
          &= 
            \begin{pmatrix}
              a_0 + a_1 + a_2 + a_3 &  0 &  0&  0\\
              0 & a_0 + a_1 j - a_2 - a_3 j &  0&  0\\
              0 &  0 & a_0 - a_1 + a_2 - a_3 &  0\\
              0 &  0 &  0& a_0 - a_1 j - a_2 + a_3 j
            \end{pmatrix}
      \end{align}
      より対角行列となる．
      ちなみに，$\matsym{F}$は離散フーリエ変換を施すDFT行列である．
      任意の巡回行列は，その要素に関わらずDFT行列により対角化される．
      すなわち，巡回行列は，DFT行列の列ベクトルを固有ベクトルとして持つ．
    \item
      \begin{equation}
        \vecsym{Y} = \matsym{F}\vecsym{y} = \matsym{F}\matsym{C}\vecsym{x} = \matsym{F}\matsym{C}\matsym{F}^{-1}\matsym{F}\vecsym{x} = \matsym{F}\matsym{C}\matsym{F}^{-1}\vecsym{X}
      \end{equation}
      より，
      $\vecsym{Y} = \matsym{F}\matsym{C}\matsym{F}^{-1}\vecsym{X}$
    \item (iii)より，$\matsym{F}\matsym{C}\matsym{F}^{-1}$は対角行列である．
      この行列の対角要素を$D_n$とおく，すなわち，
      \begin{align}
        D_1 &= a_0 + a_1 + a_2 + a_3\\
        D_2 &= a_0 + a_1 j - a_2 - a_3 j\\
        D_3 &= a_0 - a_1 + a_2 - a_3\\
        D_4 &= a_0 - a_1 j - a_2 + a_3 j
      \end{align}
      とすれば，
      (iv)より，$Y_n = D_n X_n$が成り立つ．
      このことは，時間領域の畳み込みが周波数領域の掛け算で表されることの線形代数的な理解を与える．
      ちなみに，$\vecsym{D} = (D_1, D_2, D_3, D_4), \vecsym{d} = (a_0, a_1, a_2, a_3)$とおけば，
      $\vecsym{D} = \matsym{F} \vecsym{d}$として与えられる．
    \end{enumerate}
        

  \clearpage
  \subsection{深層学習？}
    $f_n(\vecsym{x}) = \vecsym{x}$より，
    $f(\vecsym{x})$は
    \begin{equation}
      f(\vecsym{x}) = \matsym{W}_k (\matsym{W}_{k-1} (\cdots \matsym{W}_2 (\matsym{W}_1 \vecsym{x} + \vecsym{b}_1) + \vecsym{b}_2) \cdots) + \vecsym{b}_{k-1}) + \vecsym{b}_k
    \end{equation}
    となる．
    かっこを展開して整理すると
    \begin{equation}
      f(\vecsym{x}) = \matsym{W}_k \matsym{W}_{k-1} \cdots \matsym{W}_2 \matsym{W}_1 \vecsym{x} + \matsym{W}_k \matsym{W}_{k-1} \cdots \matsym{W}_2 \vecsym{b}_1 + \matsym{W}_k \matsym{W}_{k-1} \cdots \matsym{W}_3 \vecsym{b}_2 + \cdots + \matsym{W}_k \vecsym{b}_{k-1} + \vecsym{b}_k
    \end{equation}
    が得られる．
    ここで，
    $\matsym{W}_k \matsym{W}_{k-1} \cdots \matsym{W}_2 \matsym{W}_1$
    および
    $\matsym{W}_k \matsym{W}_{k-1} \cdots \matsym{W}_2 \vecsym{b}_1 + \matsym{W}_k \matsym{W}_{k-1} \cdots \matsym{W}_3 \vecsym{b}_2 + \cdots + \matsym{W}_k \vecsym{b}_{k-1} + \vecsym{b}_k$
    をそれぞれ，新たに$\matsym{W}$，$\vecsym{b}$とおくと，$f(\vecsym{x}) = \matsym{W} \vecsym{x} + \vecsym{b}$
    となる．
    これは，$f_1(\vecsym{x}) = \vecsym{x}$である1層のニューラルネットワークである．
    すなわち，$f_n(\vecsym{x}) = \vecsym{x}$のとき，
    $k$層のニューラルネットワークと等価な$1$層のニューラルネットワークが存在する．

